from huggingface_hub import snapshot_download
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# 1Ô∏è‚É£ Download the model (‚ö†Ô∏è this will likely exceed Binder limits)
model_repo = "QuantStack/Wan2.2-T2V-A14B-GGUF"

print("‚¨áÔ∏è Downloading model, this might take a while...")
model_path = snapshot_download(repo_id=model_repo)
print(f"‚úÖ Model downloaded to: {model_path}")

# 2Ô∏è‚É£ Load model to CPU (‚ö†Ô∏è likely to crash due to RAM)
print("üß† Loading model to CPU...")
model = AutoModelForCausalLM.from_pretrained(model_path, device_map="cpu")

tokenizer = AutoTokenizer.from_pretrained(model_path)

# 3Ô∏è‚É£ Run small test
text = "Once upon a time in India,"
inputs = tokenizer(text, return_tensors="pt")

print("üöÄ Running inference (this may fail on Binder)...")
outputs = model.generate(**inputs, max_new_tokens=20)
print("üó£Ô∏è Output:", tokenizer.decode(outputs[0]))
